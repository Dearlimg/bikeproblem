# éšæœºæ£®æ—ä»£ç å®ç°åŸç†è¯¦è§£

## ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•æ„é€ 100æ£µæ ‘ï¼Ÿ

æ˜¯çš„ï¼Œç¡®å®éœ€è¦ä¸€ä¸ª"å¾ªç¯"æ¥æ„é€ å¤šæ£µæ ‘ã€‚è®©æˆ‘ç”¨ä»£ç å’Œå›¾ç¤ºæ¥è¯¦ç»†è§£é‡Šã€‚

---

## ğŸ“¦ ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•´ä½“æ¶æ„

### éšæœºæ£®æ—çš„ä¼ªä»£ç 

```python
class RandomForest:
    def __init__(self, n_estimators=100):
        self.n_estimators = n_estimators  # è¦æ„é€ å¤šå°‘æ£µæ ‘
        self.trees = []  # å­˜å‚¨æ‰€æœ‰æ ‘çš„åˆ—è¡¨
    
    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—"""
        for i in range(self.n_estimators):  # å¾ªç¯100æ¬¡
            # 1. éšæœºé‡‡æ ·æ•°æ®ï¼ˆBootstrapé‡‡æ ·ï¼‰
            X_sample, y_sample = bootstrap_sample(X, y)
            
            # 2. éšæœºé€‰æ‹©ç‰¹å¾å­é›†
            features = random_feature_selection(X_sample)
            
            # 3. ç”¨é‡‡æ ·åçš„æ•°æ®å’Œç‰¹å¾è®­ç»ƒä¸€æ£µæ ‘
            tree = DecisionTree()
            tree.fit(X_sample[:, features], y_sample)
            
            # 4. æŠŠè¿™æ£µæ ‘å­˜èµ·æ¥
            self.trees.append(tree)
    
    def predict(self, X):
        """é¢„æµ‹ï¼šæ‰€æœ‰æ ‘æŠ•ç¥¨"""
        predictions = []
        for tree in self.trees:  # éå†æ‰€æœ‰æ ‘
            pred = tree.predict(X)  # æ¯æ£µæ ‘ç»™ä¸€ä¸ªé¢„æµ‹
            predictions.append(pred)
        
        # è¿”å›å¹³å‡å€¼ï¼ˆå›å½’ï¼‰æˆ–å¤šæ•°æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰
        return np.mean(predictions, axis=0)
```

**å…³é”®ç‚¹**ï¼š
- âœ… ç”¨ä¸€ä¸ª**forå¾ªç¯**æ„é€ å¤šæ£µæ ‘
- âœ… æ¯æ£µæ ‘ç”¨**ä¸åŒçš„æ•°æ®**å’Œ**ä¸åŒçš„ç‰¹å¾**è®­ç»ƒ
- âœ… é¢„æµ‹æ—¶ï¼Œæ‰€æœ‰æ ‘**æŠ•ç¥¨**ï¼Œå–å¹³å‡å€¼

---

## ğŸŒ² ç¬¬äºŒéƒ¨åˆ†ï¼šå†³ç­–æ ‘çš„æ„é€ ï¼ˆæ ¸å¿ƒï¼‰

### å†³ç­–æ ‘å¦‚ä½•"ç”Ÿé•¿"ï¼Ÿ

å†³ç­–æ ‘çš„æ„é€ æ˜¯ä¸€ä¸ª**é€’å½’è¿‡ç¨‹**ï¼Œå°±åƒ"åˆ†è€Œæ²»ä¹‹"ï¼š

```python
class DecisionTree:
    def fit(self, X, y):
        """è®­ç»ƒå†³ç­–æ ‘ï¼ˆé€’å½’ï¼‰"""
        self.root = self._build_tree(X, y)
    
    def _build_tree(self, X, y, depth=0, max_depth=10):
        """é€’å½’æ„å»ºæ ‘"""
        # 1. åœæ­¢æ¡ä»¶
        if depth >= max_depth or len(y) < 2:
            # è¿”å›å¶å­èŠ‚ç‚¹ï¼šé¢„æµ‹å€¼ä¸ºå½“å‰æ•°æ®çš„å¹³å‡å€¼
            return LeafNode(value=np.mean(y))
        
        # 2. æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        best_feature, best_threshold = self._find_best_split(X, y)
        
        # 3. æ ¹æ®æœ€ä½³åˆ†å‰²ç‚¹åˆ†å‰²æ•°æ®
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        # 4. é€’å½’æ„å»ºå·¦å³å­æ ‘
        left_tree = self._build_tree(X[left_mask], y[left_mask], depth+1)
        right_tree = self._build_tree(X[right_mask], y[right_mask], depth+1)
        
        # 5. è¿”å›å†…éƒ¨èŠ‚ç‚¹
        return InternalNode(
            feature=best_feature,
            threshold=best_threshold,
            left=left_tree,
            right=right_tree
        )
```

### å¦‚ä½•æ‰¾"æœ€ä½³åˆ†å‰²ç‚¹"ï¼Ÿ

è¿™æ˜¯å†³ç­–æ ‘çš„æ ¸å¿ƒç®—æ³•ï¼š

```python
def _find_best_split(self, X, y):
    """æ‰¾æœ€ä½³ç‰¹å¾å’Œé˜ˆå€¼"""
    best_score = float('inf')  # æœ€å¥½çš„åˆ†æ•°ï¼ˆè¯¯å·®ï¼‰
    best_feature = None
    best_threshold = None
    
    # éå†æ‰€æœ‰ç‰¹å¾
    for feature_idx in range(X.shape[1]):
        # éå†æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
        values = np.unique(X[:, feature_idx])
        
        for threshold in values:
            # å°è¯•ç”¨è¿™ä¸ªé˜ˆå€¼åˆ†å‰²
            left_mask = X[:, feature_idx] <= threshold
            right_mask = ~left_mask
            
            if left_mask.sum() == 0 or right_mask.sum() == 0:
                continue  # è·³è¿‡æ— æ•ˆåˆ†å‰²
            
            # è®¡ç®—åˆ†å‰²åçš„è¯¯å·®ï¼ˆç”¨MSEï¼‰
            left_y = y[left_mask]
            right_y = y[right_mask]
            
            # è¯¯å·® = å·¦å­æ ‘çš„MSE + å³å­æ ‘çš„MSE
            score = self._mse(left_y) + self._mse(right_y)
            
            # å¦‚æœè¿™ä¸ªåˆ†å‰²æ›´å¥½ï¼Œè®°å½•ä¸‹æ¥
            if score < best_score:
                best_score = score
                best_feature = feature_idx
                best_threshold = threshold
    
    return best_feature, best_threshold

def _mse(self, y):
    """è®¡ç®—å‡æ–¹è¯¯å·®"""
    if len(y) == 0:
        return 0
    mean = np.mean(y)
    return np.sum((y - mean) ** 2)
```

**åŸç†**ï¼š
- å°è¯•æ‰€æœ‰ç‰¹å¾å’Œæ‰€æœ‰é˜ˆå€¼
- é€‰æ‹©èƒ½è®©"åˆ†å‰²åè¯¯å·®æœ€å°"çš„åˆ†å‰²ç‚¹
- è¯¯å·®ç”¨MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰è¡¡é‡

---

## ğŸ² ç¬¬ä¸‰éƒ¨åˆ†ï¼šéšæœºæ€§å¦‚ä½•å®ç°ï¼Ÿ

### 1. Bootstrapé‡‡æ ·ï¼ˆæ•°æ®éšæœºï¼‰

```python
def bootstrap_sample(X, y, sample_size=None):
    """æœ‰æ”¾å›éšæœºé‡‡æ ·"""
    n_samples = len(X)
    if sample_size is None:
        sample_size = n_samples
    
    # éšæœºé€‰æ‹©ç´¢å¼•ï¼ˆæœ‰æ”¾å›ï¼‰
    indices = np.random.choice(n_samples, size=sample_size, replace=True)
    
    return X[indices], y[indices]
```

**æ•ˆæœ**ï¼š
- åŸå§‹æ•°æ®ï¼š1000æ¡
- é‡‡æ ·åï¼šè¿˜æ˜¯1000æ¡ï¼Œä½†æœ‰äº›æ•°æ®è¢«é€‰äº†å¤šæ¬¡ï¼Œæœ‰äº›æ²¡è¢«é€‰åˆ°
- æ¯æ£µæ ‘çœ‹åˆ°çš„æ•°æ®éƒ½ä¸åŒ

### 2. ç‰¹å¾éšæœºé€‰æ‹©

```python
def random_feature_selection(X, max_features='sqrt'):
    """éšæœºé€‰æ‹©ç‰¹å¾å­é›†"""
    n_features = X.shape[1]
    
    if max_features == 'sqrt':
        n_select = int(np.sqrt(n_features))  # é€‰sqrt(n)ä¸ªç‰¹å¾
    elif max_features == 'log2':
        n_select = int(np.log2(n_features))  # é€‰log2(n)ä¸ªç‰¹å¾
    else:
        n_select = max_features
    
    # éšæœºé€‰æ‹©ç‰¹å¾ç´¢å¼•
    selected = np.random.choice(n_features, size=n_select, replace=False)
    return selected
```

**æ•ˆæœ**ï¼š
- æ€»å…±æœ‰11ä¸ªç‰¹å¾
- æ¯æ£µæ ‘å¯èƒ½åªçœ‹å…¶ä¸­3-4ä¸ªç‰¹å¾
- è¿™æ ·æ¯æ£µæ ‘å­¦åˆ°çš„"è§†è§’"ä¸åŒ

---

## ğŸ” ç¬¬å››éƒ¨åˆ†ï¼šé¢„æµ‹è¿‡ç¨‹

### å•æ£µæ ‘çš„é¢„æµ‹

```python
def predict_tree(self, X, node):
    """ç”¨æ ‘é¢„æµ‹ï¼ˆé€’å½’ï¼‰"""
    if isinstance(node, LeafNode):
        # å¶å­èŠ‚ç‚¹ï¼šç›´æ¥è¿”å›é¢„æµ‹å€¼
        return node.value
    
    # å†…éƒ¨èŠ‚ç‚¹ï¼šæ ¹æ®ç‰¹å¾å€¼å†³å®šèµ°å·¦å­æ ‘è¿˜æ˜¯å³å­æ ‘
    if X[node.feature] <= node.threshold:
        return self.predict_tree(X, node.left)
    else:
        return self.predict_tree(X, node.right)
```

### éšæœºæ£®æ—çš„é¢„æµ‹

```python
def predict(self, X):
    """éšæœºæ£®æ—é¢„æµ‹ï¼šæ‰€æœ‰æ ‘æŠ•ç¥¨"""
    all_predictions = []
    
    for tree in self.trees:
        pred = tree.predict(X)  # æ¯æ£µæ ‘ç»™ä¸€ä¸ªé¢„æµ‹
        all_predictions.append(pred)
    
    # å›å½’ï¼šå–å¹³å‡å€¼
    return np.mean(all_predictions, axis=0)
    
    # åˆ†ç±»ï¼šå¤šæ•°æŠ•ç¥¨
    # return mode(all_predictions, axis=0)
```

---

## ğŸ“Š ç¬¬äº”éƒ¨åˆ†ï¼šå®Œæ•´ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰

è®©æˆ‘å†™ä¸€ä¸ª**æç®€ç‰ˆ**çš„éšæœºæ£®æ—ï¼Œå¸®ä½ ç†è§£ï¼š

```python
import numpy as np
from collections import Counter

class SimpleRandomForest:
    """æç®€ç‰ˆéšæœºæ£®æ—ï¼ˆç”¨äºç†è§£åŸç†ï¼‰"""
    
    def __init__(self, n_trees=100, max_depth=10, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.n_features = n_features
        self.trees = []
    
    def fit(self, X, y):
        """è®­ç»ƒ"""
        n_samples, n_features = X.shape
        
        # ç¡®å®šæ¯æ£µæ ‘ç”¨å¤šå°‘ç‰¹å¾
        if self.n_features is None:
            self.n_features = int(np.sqrt(n_features))
        
        # æ„é€ n_treesæ£µæ ‘
        for i in range(self.n_trees):
            print(f"æ­£åœ¨è®­ç»ƒç¬¬ {i+1}/{self.n_trees} æ£µæ ‘...")
            
            # 1. Bootstrapé‡‡æ ·
            indices = np.random.choice(n_samples, size=n_samples, replace=True)
            X_boot = X[indices]
            y_boot = y[indices]
            
            # 2. éšæœºé€‰æ‹©ç‰¹å¾
            feature_indices = np.random.choice(
                n_features, 
                size=self.n_features, 
                replace=False
            )
            X_boot = X_boot[:, feature_indices]
            
            # 3. è®­ç»ƒä¸€æ£µæ ‘
            tree = self._build_tree(X_boot, y_boot, feature_indices)
            self.trees.append((tree, feature_indices))
    
    def _build_tree(self, X, y, feature_indices, depth=0):
        """é€’å½’æ„å»ºæ ‘"""
        # åœæ­¢æ¡ä»¶
        if depth >= self.max_depth or len(np.unique(y)) == 1:
            return {'type': 'leaf', 'value': np.mean(y)}
        
        # æ‰¾æœ€ä½³åˆ†å‰²
        best_feature, best_threshold, best_score = self._find_best_split(X, y)
        
        if best_feature is None:
            return {'type': 'leaf', 'value': np.mean(y)}
        
        # åˆ†å‰²æ•°æ®
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        if left_mask.sum() == 0 or right_mask.sum() == 0:
            return {'type': 'leaf', 'value': np.mean(y)}
        
        # é€’å½’æ„å»ºå­æ ‘
        left_tree = self._build_tree(
            X[left_mask], y[left_mask], 
            feature_indices, depth + 1
        )
        right_tree = self._build_tree(
            X[right_mask], y[right_mask], 
            feature_indices, depth + 1
        )
        
        return {
            'type': 'node',
            'feature': feature_indices[best_feature],  # åŸå§‹ç‰¹å¾ç´¢å¼•
            'threshold': best_threshold,
            'left': left_tree,
            'right': right_tree
        }
    
    def _find_best_split(self, X, y):
        """æ‰¾æœ€ä½³åˆ†å‰²ç‚¹"""
        best_score = float('inf')
        best_feature = None
        best_threshold = None
        
        for feature_idx in range(X.shape[1]):
            values = np.unique(X[:, feature_idx])
            
            for threshold in values:
                left_mask = X[:, feature_idx] <= threshold
                right_mask = ~left_mask
                
                if left_mask.sum() == 0 or right_mask.sum() == 0:
                    continue
                
                # è®¡ç®—MSE
                left_mse = np.var(y[left_mask]) * left_mask.sum()
                right_mse = np.var(y[right_mask]) * right_mask.sum()
                score = left_mse + right_mse
                
                if score < best_score:
                    best_score = score
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_score
    
    def _predict_tree(self, x, tree):
        """ç”¨å•æ£µæ ‘é¢„æµ‹"""
        if tree['type'] == 'leaf':
            return tree['value']
        
        if x[tree['feature']] <= tree['threshold']:
            return self._predict_tree(x, tree['left'])
        else:
            return self._predict_tree(x, tree['right'])
    
    def predict(self, X):
        """é¢„æµ‹"""
        predictions = []
        
        for x in X:
            tree_preds = []
            for tree, feature_indices in self.trees:
                # åªå–è¿™æ£µæ ‘ç”¨çš„ç‰¹å¾
                x_subset = x[feature_indices]
                # éœ€è¦è°ƒæ•´æ ‘ä¸­çš„ç‰¹å¾ç´¢å¼•ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                pred = self._predict_tree_simple(x, tree)
                tree_preds.append(pred)
            
            # å–å¹³å‡å€¼
            predictions.append(np.mean(tree_preds))
        
        return np.array(predictions)
    
    def _predict_tree_simple(self, x, tree):
        """ç®€åŒ–ç‰ˆæ ‘é¢„æµ‹ï¼ˆå‡è®¾ç‰¹å¾ç´¢å¼•å·²è°ƒæ•´ï¼‰"""
        if tree['type'] == 'leaf':
            return tree['value']
        
        if x[tree['feature']] <= tree['threshold']:
            return self._predict_tree_simple(x, tree['left'])
        else:
            return self._predict_tree_simple(x, tree['right'])
```

---

## ğŸ¯ ç¬¬å…­éƒ¨åˆ†ï¼šsklearnçš„å®ç°ï¼ˆå®é™…ä½¿ç”¨ï¼‰

æˆ‘ä»¬ä»£ç ä¸­ç”¨çš„æ˜¯sklearnçš„`RandomForestRegressor`ï¼Œå®ƒçš„å®ç°æ›´å¤æ‚ä½†åŸç†ç›¸åŒï¼š

```python
from sklearn.ensemble import RandomForestRegressor

# æˆ‘ä»¬ä»£ç ä¸­çš„ä½¿ç”¨
model = RandomForestRegressor(
    n_estimators=100,      # æ„é€ 100æ£µæ ‘
    max_depth=10,          # æ¯æ£µæ ‘æœ€å¤§æ·±åº¦10
    random_state=42,       # éšæœºç§å­
    n_jobs=-1             # å¹¶è¡Œè®¡ç®—ï¼ˆç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰
)

# sklearnå†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
class RandomForestRegressor:
    def __init__(self, n_estimators=100, max_depth=10):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.estimators_ = []  # å­˜å‚¨æ‰€æœ‰æ ‘
    
    def fit(self, X, y):
        for i in range(self.n_estimators):
            # Bootstrapé‡‡æ ·
            indices = np.random.choice(len(X), len(X), replace=True)
            X_boot, y_boot = X[indices], y[indices]
            
            # è®­ç»ƒä¸€æ£µæ ‘ï¼ˆç”¨Cythonä¼˜åŒ–è¿‡çš„CARTç®—æ³•ï¼‰
            tree = DecisionTreeRegressor(max_depth=self.max_depth)
            tree.fit(X_boot, y_boot)
            self.estimators_.append(tree)
    
    def predict(self, X):
        # æ‰€æœ‰æ ‘é¢„æµ‹ï¼Œç„¶åå¹³å‡
        predictions = np.array([tree.predict(X) for tree in self.estimators_])
        return np.mean(predictions, axis=0)
```

**sklearnçš„ä¼˜åŠ¿**ï¼š
- âœ… ç”¨Cythonä¼˜åŒ–ï¼Œé€Ÿåº¦å¾ˆå¿«
- âœ… æ”¯æŒå¹¶è¡Œè®¡ç®—ï¼ˆ`n_jobs=-1`ï¼‰
- âœ… æœ‰å¾ˆå¤šå‚æ•°å¯ä»¥è°ƒä¼˜
- âœ… ä»£ç ç»è¿‡å¤§é‡æµ‹è¯•ï¼Œç¨³å®šå¯é 

---

## ğŸ”„ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®Œæ•´æµç¨‹å›¾

```
å¼€å§‹è®­ç»ƒéšæœºæ£®æ—
    â†“
for i in range(100):  â† å¾ªç¯100æ¬¡
    â†“
[ç¬¬1æ£µæ ‘]
â”œâ”€ Bootstrapé‡‡æ ·æ•°æ®ï¼ˆæœ‰æ”¾å›ï¼‰
â”œâ”€ éšæœºé€‰æ‹©ç‰¹å¾å­é›†
â”œâ”€ è®­ç»ƒå†³ç­–æ ‘ï¼ˆé€’å½’æ„å»ºï¼‰
â””â”€ ä¿å­˜æ ‘
    â†“
[ç¬¬2æ£µæ ‘]
â”œâ”€ Bootstrapé‡‡æ ·æ•°æ®ï¼ˆä¸åŒçš„æ ·æœ¬ï¼‰
â”œâ”€ éšæœºé€‰æ‹©ç‰¹å¾å­é›†ï¼ˆä¸åŒçš„ç‰¹å¾ï¼‰
â”œâ”€ è®­ç»ƒå†³ç­–æ ‘
â””â”€ ä¿å­˜æ ‘
    â†“
...
    â†“
[ç¬¬100æ£µæ ‘]
    â†“
è®­ç»ƒå®Œæˆï¼ç°åœ¨æœ‰100æ£µæ ‘äº†
    â†“
é¢„æµ‹æ—¶ï¼š
â”œâ”€ è¾“å…¥æ–°æ•°æ®
â”œâ”€ æ¯æ£µæ ‘éƒ½é¢„æµ‹ä¸€æ¬¡
â”œâ”€ æ”¶é›†100ä¸ªé¢„æµ‹å€¼
â””â”€ è¿”å›å¹³å‡å€¼
```

---

## ğŸ’¡ å…³é”®ç‚¹æ€»ç»“

1. **å¾ªç¯æ„é€ æ ‘**ï¼šç”¨ä¸€ä¸ª`for`å¾ªç¯ï¼Œæ„é€ `n_estimators`æ£µæ ‘
2. **æ¯æ£µæ ‘ä¸åŒ**ï¼š
   - æ•°æ®ä¸åŒï¼ˆBootstrapé‡‡æ ·ï¼‰
   - ç‰¹å¾ä¸åŒï¼ˆéšæœºç‰¹å¾é€‰æ‹©ï¼‰
3. **å†³ç­–æ ‘é€’å½’**ï¼šæ¯æ£µæ ‘ç”¨é€’å½’ç®—æ³•æ„å»ºï¼Œä¸æ–­"åˆ†è€Œæ²»ä¹‹"
4. **é¢„æµ‹å–å¹³å‡**ï¼šæ‰€æœ‰æ ‘æŠ•ç¥¨ï¼Œå›å½’é—®é¢˜å–å¹³å‡å€¼

---

## ğŸ“ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

### 1. ä¸ºä»€ä¹ˆç”¨Bootstrapé‡‡æ ·ï¼Ÿ
- **å¢åŠ å¤šæ ·æ€§**ï¼šæ¯æ£µæ ‘çœ‹åˆ°çš„æ•°æ®ä¸åŒ
- **å‡å°‘è¿‡æ‹Ÿåˆ**ï¼šä¸æ˜¯æ‰€æœ‰æ•°æ®éƒ½è¢«æ¯æ£µæ ‘çœ‹åˆ°
- **æ¨¡æ‹Ÿ"æŠ•ç¥¨"**ï¼šå°±åƒ100ä¸ªäººçœ‹ä¸åŒçš„æ•°æ®ï¼Œç„¶åæŠ•ç¥¨

### 2. ä¸ºä»€ä¹ˆéšæœºé€‰ç‰¹å¾ï¼Ÿ
- **å‡å°‘ç›¸å…³æ€§**ï¼šå¦‚æœæ‰€æœ‰æ ‘éƒ½ç”¨ç›¸åŒç‰¹å¾ï¼Œå®ƒä»¬ä¼šå¾ˆç›¸ä¼¼
- **æé«˜æ³›åŒ–**ï¼šä¸åŒè§†è§’çœ‹é—®é¢˜ï¼Œç»¼åˆèµ·æ¥æ›´å‡†ç¡®
- **é™ä½æ–¹å·®**ï¼šå‡å°‘æ¨¡å‹å¯¹ç‰¹å®šç‰¹å¾çš„ä¾èµ–

### 3. ä¸ºä»€ä¹ˆå–å¹³å‡å€¼ï¼Ÿ
- **é™ä½è¯¯å·®**ï¼šå•æ£µæ ‘å¯èƒ½é¢„æµ‹ä¸å‡†ï¼Œä½†100æ£µæ ‘å¹³å‡ï¼Œè¯¯å·®ä¼šæŠµæ¶ˆ
- **æ›´ç¨³å®š**ï¼šå¹³å‡å€¼æ¯”å•ä¸ªå€¼æ›´ç¨³å®š
- **æ•°å­¦è¯æ˜**ï¼šç†è®ºä¸Šï¼Œé›†æˆå­¦ä¹ èƒ½é™ä½æ–¹å·®

---

å¸Œæœ›è¿™ä¸ªè§£é‡Šå¸®ä½ ç†è§£äº†éšæœºæ£®æ—çš„ä»£ç å®ç°åŸç†ï¼æ ¸å¿ƒå°±æ˜¯ï¼š**å¾ªç¯æ„é€ å¤šæ£µæ ‘ï¼Œæ¯æ£µæ ‘ç”¨ä¸åŒçš„æ•°æ®å’Œç‰¹å¾è®­ç»ƒï¼Œé¢„æµ‹æ—¶å–å¹³å‡**ã€‚

