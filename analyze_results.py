#!/usr/bin/env python3
"""
ç»“æœåˆ†æè„šæœ¬
è¯¦ç»†åˆ†ææ¨¡å‹è®­ç»ƒç»“æœå¹¶æä¾›æ·±å…¥è§è§£
"""

import sys
import os
import pandas as pd
import numpy as np

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.data_loader import DataLoader
from src.data_preprocessor import DataPreprocessor
from src.model_trainer import ModelTrainer
from src.visualizer import Visualizer


def analyze_model_performance(results):
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    print("\n" + "="*70)
    print("æ¨¡å‹æ€§èƒ½è¯¦ç»†åˆ†æ")
    print("="*70)
    
    for model_name, result in results.items():
        print(f"\nã€{model_name}ã€‘")
        print("-" * 70)
        
        train_metrics = result['train_metrics']
        test_metrics = result['test_metrics']
        
        # è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦
        train_r2 = train_metrics['r2_score']
        test_r2 = test_metrics['r2_score']
        overfitting = train_r2 - test_r2
        
        print(f"è®­ç»ƒé›†æ€§èƒ½:")
        print(f"  RÂ² åˆ†æ•°: {train_r2:.4f}")
        print(f"  RMSE: {train_metrics['rmse']:.2f}")
        print(f"  MAE: {train_metrics['mae']:.2f}")
        
        print(f"\næµ‹è¯•é›†æ€§èƒ½:")
        print(f"  RÂ² åˆ†æ•°: {test_r2:.4f}")
        print(f"  RMSE: {test_metrics['rmse']:.2f}")
        print(f"  MAE: {test_metrics['mae']:.2f}")
        
        print(f"\nè¿‡æ‹Ÿåˆåˆ†æ:")
        if overfitting < 0.05:
            print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f} (è½»å¾®ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½)")
        elif overfitting < 0.15:
            print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f} (ä¸­ç­‰ï¼Œæ¨¡å‹å¯èƒ½ç•¥æœ‰è¿‡æ‹Ÿåˆ)")
        else:
            print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f} (ä¸¥é‡ï¼Œæ¨¡å‹å­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ)")
        
        # é¢„æµ‹è¯¯å·®åˆ†æ
        y_test = result['y_test']
        y_pred = result['y_test_pred']
        errors = np.abs(y_test - y_pred)
        relative_errors = errors / (y_test + 1) * 100  # é¿å…é™¤é›¶
        
        print(f"\né¢„æµ‹è¯¯å·®åˆ†æ:")
        print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {errors.mean():.2f}")
        print(f"  è¯¯å·®ä¸­ä½æ•°: {np.median(errors):.2f}")
        print(f"  æœ€å¤§è¯¯å·®: {errors.max():.2f}")
        print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {relative_errors.mean():.2f}%")
        print(f"  è¯¯å·®æ ‡å‡†å·®: {errors.std():.2f}")


def analyze_feature_importance(trainer, preprocessor):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    print("\n" + "="*70)
    print("ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("="*70)
    
    if trainer.best_model is None:
        print("æ¨¡å‹å°šæœªè®­ç»ƒ")
        return
    
    feature_importance = trainer.get_feature_importance()
    importance_df = preprocessor.get_feature_importance_data(feature_importance)
    
    print(f"\næœ€ä½³æ¨¡å‹: {trainer.best_model_name}")
    print("\nç‰¹å¾é‡è¦æ€§æ’åº (ä»é«˜åˆ°ä½):")
    print("-" * 70)
    
    total_importance = importance_df['importance'].sum()
    
    for idx, row in importance_df.iterrows():
        importance_pct = (row['importance'] / total_importance) * 100
        bar_length = int(importance_pct / 2)  # æ¯2%ä¸€ä¸ªå­—ç¬¦
        bar = "â–ˆ" * bar_length
        print(f"{row['feature']:15s} | {bar:50s} | {row['importance']:.4f} ({importance_pct:.1f}%)")
    
    # åˆ†ææœ€é‡è¦çš„ç‰¹å¾
    top_features = importance_df.head(5)
    print(f"\næœ€é‡è¦çš„5ä¸ªç‰¹å¾:")
    for idx, row in top_features.iterrows():
        print(f"  {idx+1}. {row['feature']}: {row['importance']:.4f}")


def analyze_prediction_quality(results):
    """åˆ†æé¢„æµ‹è´¨é‡"""
    print("\n" + "="*70)
    print("é¢„æµ‹è´¨é‡åˆ†æ")
    print("="*70)
    
    for model_name, result in results.items():
        print(f"\nã€{model_name}ã€‘")
        print("-" * 70)
        
        y_test = result['y_test']
        y_pred = result['y_test_pred']
        
        # æŒ‰çœŸå®å€¼èŒƒå›´åˆ†æè¯¯å·®
        ranges = [
            (0, 2000, "ä½éœ€æ±‚ (0-2000)"),
            (2000, 4000, "ä¸­ä½éœ€æ±‚ (2000-4000)"),
            (4000, 6000, "ä¸­é«˜éœ€æ±‚ (4000-6000)"),
            (6000, float('inf'), "é«˜éœ€æ±‚ (6000+)")
        ]
        
        print("ä¸åŒéœ€æ±‚æ°´å¹³çš„é¢„æµ‹è¯¯å·®:")
        for min_val, max_val, label in ranges:
            mask = (y_test >= min_val) & (y_test < max_val)
            if mask.sum() > 0:
                errors = np.abs(y_test[mask] - y_pred[mask])
                mae = errors.mean()
                mape = (errors / (y_test[mask] + 1) * 100).mean()
                print(f"  {label:25s}: MAE={mae:.2f}, MAPE={mape:.2f}%, æ ·æœ¬æ•°={mask.sum()}")


def compare_models(results):
    """å¯¹æ¯”æ¨¡å‹"""
    print("\n" + "="*70)
    print("æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print("="*70)
    
    comparison_data = []
    for model_name, result in results.items():
        test_metrics = result['test_metrics']
        train_metrics = result['train_metrics']
        comparison_data.append({
            'æ¨¡å‹': model_name,
            'æµ‹è¯•é›†RÂ²': test_metrics['r2_score'],
            'æµ‹è¯•é›†RMSE': test_metrics['rmse'],
            'æµ‹è¯•é›†MAE': test_metrics['mae'],
            'è®­ç»ƒé›†RÂ²': train_metrics['r2_score'],
            'è¿‡æ‹Ÿåˆç¨‹åº¦': train_metrics['r2_score'] - test_metrics['r2_score']
        })
    
    df = pd.DataFrame(comparison_data)
    print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨:")
    print(df.to_string(index=False))
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_r2 = df.loc[df['æµ‹è¯•é›†RÂ²'].idxmax()]
    best_rmse = df.loc[df['æµ‹è¯•é›†RMSE'].idxmin()]
    best_mae = df.loc[df['æµ‹è¯•é›†MAE'].idxmin()]
    
    print(f"\næœ€ä½³RÂ²åˆ†æ•°: {best_r2['æ¨¡å‹']} ({best_r2['æµ‹è¯•é›†RÂ²']:.4f})")
    print(f"æœ€ä½RMSE: {best_rmse['æ¨¡å‹']} ({best_rmse['æµ‹è¯•é›†RMSE']:.2f})")
    print(f"æœ€ä½MAE: {best_mae['æ¨¡å‹']} ({best_mae['æµ‹è¯•é›†MAE']:.2f})")


def generate_recommendations(results, trainer):
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    print("\n" + "="*70)
    print("æ”¹è¿›å»ºè®®")
    print("="*70)
    
    recommendations = []
    
    # æ£€æŸ¥è¿‡æ‹Ÿåˆ
    for model_name, result in results.items():
        train_r2 = result['train_metrics']['r2_score']
        test_r2 = result['test_metrics']['r2_score']
        overfitting = train_r2 - test_r2
        
        if overfitting > 0.15:
            recommendations.append(f"âš ï¸  {model_name}å­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ(å·®å¼‚{overfitting:.3f})ï¼Œå»ºè®®:")
            recommendations.append("   - å¢åŠ æ­£åˆ™åŒ–å‚æ•°")
            recommendations.append("   - å‡å°‘æ¨¡å‹å¤æ‚åº¦")
            recommendations.append("   - å¢åŠ è®­ç»ƒæ•°æ®æˆ–ä½¿ç”¨äº¤å‰éªŒè¯")
    
    # æ£€æŸ¥æ¨¡å‹æ€§èƒ½
    best_result = results[trainer.best_model_name]
    best_r2 = best_result['test_metrics']['r2_score']
    
    if best_r2 < 0.7:
        recommendations.append("âš ï¸  æ¨¡å‹RÂ²åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®:")
        recommendations.append("   - è¿›è¡Œæ›´æ·±å…¥çš„ç‰¹å¾å·¥ç¨‹")
        recommendations.append("   - å°è¯•æ›´å¤æ‚çš„æ¨¡å‹(å¦‚XGBoostã€ç¥ç»ç½‘ç»œ)")
        recommendations.append("   - æ£€æŸ¥æ•°æ®è´¨é‡å’Œç‰¹å¾é€‰æ‹©")
    elif best_r2 < 0.85:
        recommendations.append("ğŸ’¡ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´:")
        recommendations.append("   - å°è¯•ç‰¹å¾äº¤äº’é¡¹")
        recommendations.append("   - è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
        recommendations.append("   - å°è¯•é›†æˆå­¦ä¹ æ–¹æ³•")
    else:
        recommendations.append("âœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€!")
        recommendations.append("   - å¯ä»¥è€ƒè™‘æ¨¡å‹éƒ¨ç½²")
        recommendations.append("   - å¯ä»¥å°è¯•æ¨¡å‹å‹ç¼©ä»¥æå‡æ¨ç†é€Ÿåº¦")
    
    # ç‰¹å¾å·¥ç¨‹å»ºè®®
    if trainer.best_model is not None:
        feature_importance = trainer.get_feature_importance()
        if len(feature_importance) > 0:
            max_importance = feature_importance.max()
            min_importance = feature_importance.min()
            if max_importance / min_importance > 100:
                recommendations.append("ğŸ’¡ ç‰¹å¾é‡è¦æ€§å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®:")
                recommendations.append("   - è€ƒè™‘ç§»é™¤é‡è¦æ€§æä½çš„ç‰¹å¾")
                recommendations.append("   - å¯¹é‡è¦ç‰¹å¾è¿›è¡Œæ›´ç²¾ç»†çš„ç‰¹å¾å·¥ç¨‹")
    
    if recommendations:
        for rec in recommendations:
            print(rec)
    else:
        print("æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæš‚æ— ç‰¹æ®Šå»ºè®®ã€‚")


def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("="*70)
    print("å…±äº«å•è½¦ç§Ÿèµé¢„æµ‹ - ç»“æœåˆ†ææŠ¥å‘Š")
    print("="*70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[æ­¥éª¤ 1] åŠ è½½æ•°æ®...")
    data_loader = DataLoader(data_dir="data")
    # df = data_loader.load_day_data()
    df = data_loader.load_hour_data()
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("\n[æ­¥éª¤ 2] æ•°æ®é¢„å¤„ç†...")
    preprocessor = DataPreprocessor()
    X, y = preprocessor.prepare_features(df, target="cnt")
    X_scaled, _ = preprocessor.scale_features(X)
    
    # 3. æ¨¡å‹è®­ç»ƒ
    print("\n[æ­¥éª¤ 3] æ¨¡å‹è®­ç»ƒ...")
    trainer = ModelTrainer(random_state=42)
    results = trainer.train_models(X_scaled, y, test_size=0.2)
    
    # 4. è¯¦ç»†åˆ†æ
    analyze_model_performance(results)
    analyze_feature_importance(trainer, preprocessor)
    analyze_prediction_quality(results)
    compare_models(results)
    generate_recommendations(results, trainer)
    
    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()

